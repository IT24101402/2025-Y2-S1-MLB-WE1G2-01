{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:49:18.392496Z",
     "start_time": "2025-09-23T18:49:18.383405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ],
   "id": "f89c5072961b050a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:51:44.339300Z",
     "start_time": "2025-09-23T18:51:44.330520Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = \"Tea Leaf Disease Dataset\"",
   "id": "f88e21b652e9c28e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:56:05.770993Z",
     "start_time": "2025-09-23T18:56:05.553903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import os, csv\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IMAGE_EXTS = (\".jpg\", \".png\")  # what you kept after cleaning\n",
    "\n",
    "def list_images_with_labels(root_dir):\n",
    "    paths, labels = [], []\n",
    "    for cls in sorted(os.listdir(root_dir)):\n",
    "        cdir = os.path.join(root_dir, cls)\n",
    "        if not os.path.isdir(cdir) or cls.startswith(\"_\"):\n",
    "            continue\n",
    "        for f in sorted(os.listdir(cdir)):\n",
    "            if f.lower().endswith(IMAGE_EXTS):\n",
    "                # store relative path (to dataset root)\n",
    "                rel = os.path.join(cls, f)\n",
    "                paths.append(rel)\n",
    "                labels.append(cls)\n",
    "    return paths, labels\n",
    "\n",
    "def write_index_csv(rows, out_csv):\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"path\",\"label\"]); w.writerows(rows)\n",
    "\n",
    "def stratified_split_index(data_dir, out_dir, test_size=0.20, val_size=0.10, random_state=42):\n",
    "    X, y = list_images_with_labels(data_dir)\n",
    "    if not X:\n",
    "        raise FileNotFoundError(f\"No images found under {data_dir} with {IMAGE_EXTS}\")\n",
    "    counts = Counter(y)\n",
    "    if any(v < 2 for v in counts.values()):\n",
    "        raise ValueError(f\"Some classes too small: {counts}\")\n",
    "\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(\n",
    "        X, y, test_size=test_size+val_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    rel_val = val_size / (test_size+val_size)\n",
    "    X_val, X_te, y_val, y_te = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=1-rel_val, stratify=y_tmp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    write_index_csv(list(zip(X_tr,y_tr)), os.path.join(out_dir,\"train_index.csv\"))\n",
    "    write_index_csv(list(zip(X_val,y_val)), os.path.join(out_dir,\"val_index.csv\"))\n",
    "    write_index_csv(list(zip(X_te,y_te)), os.path.join(out_dir,\"test_index.csv\"))\n",
    "\n",
    "    print(f\"Train: {len(X_tr)}, Val: {len(X_val)}, Test: {len(X_te)}\")\n",
    "\n",
    "# --- Resolve dataset path from Notebooks/ ---\n",
    "cwd = Path.cwd()\n",
    "candidates = [\n",
    "    cwd / \"Tea Leaf Disease Dataset\",\n",
    "    cwd.parent / \"Tea Leaf Disease Dataset\",                # <â€” your case\n",
    "    cwd / \"Data\" / \"Raw\" / \"Tea Leaf Disease Dataset\",\n",
    "    cwd.parent / \"Data\" / \"Raw\" / \"Tea Leaf Disease Dataset\",\n",
    "]\n",
    "dataset_dir = next((p for p in candidates if p.is_dir()), None)\n",
    "if dataset_dir is None:\n",
    "    raise FileNotFoundError(\"Couldn't find dataset. Tried:\\n\" + \"\\n\".join(str(p) for p in candidates))\n",
    "print(\"Using dataset at:\", dataset_dir)\n",
    "\n",
    "# Run\n",
    "stratified_split_index(str(dataset_dir), \"splits\", test_size=0.20, val_size=0.10)"
   ],
   "id": "3c256112291365d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset at: /Users/tharukakumarasiri/Desktop/AI_ML_Project/Notebooks/Tea Leaf Disease Dataset\n",
      "Train: 22694, Val: 3242, Test: 6485\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T19:07:33.499726Z",
     "start_time": "2025-09-23T19:06:47.550493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === TEA DATASET QUALITY CONTROL & CLEAN (final) ===\n",
    "# Works from project root OR from Notebooks/. Saves a CSV log.\n",
    "\n",
    "import os, csv, hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "# Where is the dataset? These two cover both typical run locations:\n",
    "CANDIDATES = [\n",
    "    Path(\"Tea Leaf Disease Dataset\"),\n",
    "    Path(\"../Tea Leaf Disease Dataset\"),\n",
    "    Path(\"Data/Raw/Tea Leaf Disease Dataset\"),\n",
    "    Path(\"../Data/Raw/Tea Leaf Disease Dataset\"),\n",
    "]\n",
    "\n",
    "# Formats you will actually use for training\n",
    "ALLOWED_EXTS   = (\".jpg\", \".png\")\n",
    "# What we scan for (so we can catch disallowed ones)\n",
    "SCAN_EXTS      = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\")\n",
    "# Treat these as \"corrupt/disallowed\" on sight\n",
    "DISALLOWED_EXTS = (\".bmp\", \".webp\", \".tif\", \".tiff\", \".jpeg\")\n",
    "\n",
    "# Structural/image-quality constraints\n",
    "MIN_SIDE   = 64           # remove if min(height,width) < MIN_SIDE\n",
    "MAX_ASPECT = 3.0          # remove if max/min side ratio > MAX_ASPECT\n",
    "REQUIRE_RGB = True        # remove non-RGB (e.g., RGBA, L, CMYK)\n",
    "\n",
    "# Duplicates\n",
    "CHECK_EXACT_DUPLICATES   = True   # MD5 exact files\n",
    "CHECK_NEAR_DUP_SAME_PHASH = True  # remove if SAME perceptual hash bucket (aggressive)\n",
    "PHASH_SIZE = 8                     # aHash grid size (8x8 -> 64 bits)\n",
    "\n",
    "# Action\n",
    "DRY_RUN  = True                    # True: report only; False: apply ACTION\n",
    "ACTION   = \"quarantine\"            # \"quarantine\" or \"delete\"\n",
    "# ----------------------------------------\n",
    "\n",
    "# Optional: allow truncated files to load (usually keep False)\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --------- Path resolve ----------\n",
    "def resolve_dataset_dir() -> Path:\n",
    "    for p in CANDIDATES:\n",
    "        if p.is_dir():\n",
    "            return p.resolve()\n",
    "    tried = \"\\n - \".join(str(p.resolve()) for p in CANDIDATES)\n",
    "    raise FileNotFoundError(\n",
    "        \"Couldn't find the dataset folder.\\n\"\n",
    "        f\"CWD: {Path.cwd()}\\nTried:\\n - {tried}\\n\"\n",
    "        \"Tip: if running from Notebooks/, '../Tea Leaf Disease Dataset' is typical.\"\n",
    "    )\n",
    "\n",
    "DATASET_DIR = resolve_dataset_dir()\n",
    "QUARANTINE_DIR = DATASET_DIR / \"_quarantine_corrupt\"\n",
    "LOG_CSV = DATASET_DIR / \"_clean_log.csv\"\n",
    "if ACTION == \"quarantine\":\n",
    "    QUARANTINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------- Helpers ----------\n",
    "def move_or_delete(src: Path):\n",
    "    if DRY_RUN:\n",
    "        return None\n",
    "    if ACTION == \"delete\":\n",
    "        src.unlink(missing_ok=True)\n",
    "        return None\n",
    "    # quarantine (preserve class subfolders)\n",
    "    rel = src.relative_to(DATASET_DIR)\n",
    "    dst = QUARANTINE_DIR / rel\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    base, ext = os.path.splitext(dst)\n",
    "    k, final = 1, dst\n",
    "    while Path(final).exists():\n",
    "        final = Path(f\"{base}__dup{k}{ext}\")\n",
    "        k += 1\n",
    "    src.replace(final)\n",
    "    return str(final)\n",
    "\n",
    "def file_md5(path: Path, chunk=8192) -> str:\n",
    "    m = hashlib.md5()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for blk in iter(lambda: f.read(chunk), b\"\"):\n",
    "            m.update(blk)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def phash_hex(path: Path, hash_size=PHASH_SIZE) -> str:\n",
    "    # Simple average hash (aHash) as hex\n",
    "    with Image.open(path) as im:\n",
    "        im = im.convert(\"L\").resize((hash_size, hash_size), Image.LANCZOS)\n",
    "        pixels = list(im.getdata())\n",
    "        avg = sum(pixels) / len(pixels)\n",
    "        bits = ''.join('1' if p > avg else '0' for p in pixels)\n",
    "        return f\"{int(bits, 2):016x}\"\n",
    "\n",
    "# --------- Main scan ----------\n",
    "issues = []                  # [class, path, reason, action]\n",
    "before_counts = {}\n",
    "after_counts  = {}\n",
    "per_class_errors = defaultdict(int)\n",
    "\n",
    "all_valid_files = []         # survivors of pass 1\n",
    "class_of = {}                # map path -> class\n",
    "\n",
    "for cls in sorted(os.listdir(DATASET_DIR)):\n",
    "    cls_path = DATASET_DIR / cls\n",
    "    if not cls_path.is_dir() or cls.startswith(\"_\"):\n",
    "        continue\n",
    "\n",
    "    files = [f for f in os.listdir(cls_path) if f.lower().endswith(SCAN_EXTS)]\n",
    "    before_counts[cls] = len(files)\n",
    "    valid = 0\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = cls_path / fname\n",
    "        ext = fpath.suffix.lower()\n",
    "\n",
    "        # 0) disallowed extension\n",
    "        if ext in DISALLOWED_EXTS:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"Disallowed extension: {ext}\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        # 1) zero-byte\n",
    "        if fpath.stat().st_size == 0:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), \"Zero-byte file\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        # 2) readability + full decode\n",
    "        try:\n",
    "            with Image.open(fpath) as im:\n",
    "                im.verify()\n",
    "            with Image.open(fpath) as im:\n",
    "                im.load()\n",
    "                mode = im.mode\n",
    "                w, h = im.size\n",
    "        except UnidentifiedImageError:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), \"Unidentified/unsupported image\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "        except OSError as e:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"OSError: {e}\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"Other error: {type(e).__name__}: {e}\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        # 3) mode rule\n",
    "        if REQUIRE_RGB and mode != \"RGB\":\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"Invalid mode: {mode} (need RGB)\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        # 4) size/aspect rules\n",
    "        if min(w, h) < MIN_SIDE:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"Too small: {w}x{h} (min<{MIN_SIDE})\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        ar = max(w, h) / max(1, min(w, h))\n",
    "        if ar > MAX_ASPECT:\n",
    "            per_class_errors[cls] += 1\n",
    "            dst = move_or_delete(fpath)\n",
    "            issues.append([cls, str(fpath), f\"Extreme aspect ratio: {w}x{h} (>{MAX_ASPECT}:1)\", dst or \"DRY_RUN\"])\n",
    "            continue\n",
    "\n",
    "        # Survived pass 1\n",
    "        valid += 1\n",
    "        all_valid_files.append(fpath)\n",
    "        class_of[fpath] = cls\n",
    "\n",
    "    after_counts[cls] = valid\n",
    "\n",
    "# --------- Duplicates ----------\n",
    "# Exact duplicates by MD5 (within class)\n",
    "if CHECK_EXACT_DUPLICATES and all_valid_files:\n",
    "    by_class = defaultdict(list)\n",
    "    for p in all_valid_files:\n",
    "        if p.exists():\n",
    "            by_class[class_of[p]].append(p)\n",
    "    for cls, paths in by_class.items():\n",
    "        md5_map = defaultdict(list)\n",
    "        for p in paths:\n",
    "            try:\n",
    "                md5_map[file_md5(p)].append(p)\n",
    "            except Exception as e:\n",
    "                per_class_errors[cls] += 1\n",
    "                dst = move_or_delete(p)\n",
    "                issues.append([cls, str(p), f\"MD5 failed: {e}\", dst or \"DRY_RUN\"])\n",
    "                after_counts[cls] -= 1\n",
    "\n",
    "        for group in md5_map.values():\n",
    "            if len(group) > 1:\n",
    "                keep = group[0]\n",
    "                for g in group[1:]:\n",
    "                    if not g.exists():\n",
    "                        continue\n",
    "                    per_class_errors[cls] += 1\n",
    "                    dst = move_or_delete(g)\n",
    "                    issues.append([cls, str(g), f\"Exact duplicate of {keep.relative_to(DATASET_DIR)}\", dst or \"DRY_RUN\"])\n",
    "                    after_counts[cls] -= 1\n",
    "\n",
    "# Near-duplicates by identical pHash (aggressive; within class)\n",
    "if CHECK_NEAR_DUP_SAME_PHASH and all_valid_files:\n",
    "    by_class = defaultdict(list)\n",
    "    for p in all_valid_files:\n",
    "        if p.exists():\n",
    "            by_class[class_of[p]].append(p)\n",
    "\n",
    "    for cls, paths in by_class.items():\n",
    "        buckets = defaultdict(list)\n",
    "        for p in paths:\n",
    "            try:\n",
    "                h = phash_hex(p)\n",
    "                buckets[h].append(p)\n",
    "            except Exception as e:\n",
    "                per_class_errors[cls] += 1\n",
    "                dst = move_or_delete(p)\n",
    "                issues.append([cls, str(p), f\"pHash failed: {e}\", dst or \"DRY_RUN\"])\n",
    "                after_counts[cls] -= 1\n",
    "\n",
    "        for bucket_paths in buckets.values():\n",
    "            if len(bucket_paths) <= 1:\n",
    "                continue\n",
    "            keep = bucket_paths[0]\n",
    "            for g in bucket_paths[1:]:\n",
    "                if not g.exists():\n",
    "                    continue\n",
    "                per_class_errors[cls] += 1\n",
    "                dst = move_or_delete(g)\n",
    "                issues.append([cls, str(g), f\"Near-duplicate of {keep.relative_to(DATASET_DIR)} (same pHash)\", dst or \"DRY_RUN\"])\n",
    "                after_counts[cls] -= 1\n",
    "\n",
    "# --------- Report ----------\n",
    "removed_total = sum(before_counts[c] - after_counts[c] for c in before_counts)\n",
    "\n",
    "print(\"Dataset root:\", DATASET_DIR)\n",
    "print(\"\\n=== Summary per class ===\")\n",
    "print(\"{:<30} {:>8} {:>8} {:>8}\".format(\"Class\", \"Before\", \"Valid\", \"Removed\"))\n",
    "print(\"-\"*60)\n",
    "for cls in sorted(before_counts):\n",
    "    removed = before_counts[cls] - after_counts[cls]\n",
    "    print(\"{:<30} {:>8} {:>8} {:>8}\".format(cls, before_counts[cls], after_counts[cls], removed))\n",
    "\n",
    "print(\"\\nTotal removed:\", removed_total)\n",
    "print(\"Mode:\", \"DRY RUN (no files moved/deleted)\" if DRY_RUN else f\"ACTION = {ACTION.upper()}\")\n",
    "if ACTION == \"quarantine\":\n",
    "    print(\"Quarantine folder:\", QUARANTINE_DIR)\n",
    "\n",
    "# Save CSV audit log\n",
    "with LOG_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"class\", \"path\", \"reason\", \"action\"])\n",
    "    for cls, path, reason, action in issues:\n",
    "        w.writerow([cls, path, reason, action])\n",
    "print(\"Log saved to:\", LOG_CSV)"
   ],
   "id": "6c8a478f282fbc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /Users/tharukakumarasiri/Desktop/AI_ML_Project/Notebooks/Tea Leaf Disease Dataset\n",
      "\n",
      "=== Summary per class ===\n",
      "Class                            Before    Valid  Removed\n",
      "------------------------------------------------------------\n",
      "algal_spot                         5497     5135      362\n",
      "brown_blight                       4908     4810       98\n",
      "gray_blight                        5537     5121      416\n",
      "healthy                            5492     4990      502\n",
      "helopeltis                         5482     5254      228\n",
      "red_spot                           5505     5266      239\n",
      "\n",
      "Total removed: 1845\n",
      "Mode: DRY RUN (no files moved/deleted)\n",
      "Quarantine folder: /Users/tharukakumarasiri/Desktop/AI_ML_Project/Notebooks/Tea Leaf Disease Dataset/_quarantine_corrupt\n",
      "Log saved to: /Users/tharukakumarasiri/Desktop/AI_ML_Project/Notebooks/Tea Leaf Disease Dataset/_clean_log.csv\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
